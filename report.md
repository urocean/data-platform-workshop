
# Отчет по проекту "Разработка прототипа платформы данных для интеллектуальной рекомендательной системы"

**Студент:** Осина Виктория  
**Дата:** 21 февраля 2026  
**ОС:** Rocky Linux 9.5  
**GitHub:** https://github.com/urocean/data-platform-workshop

---

## 1. Облачное хранилище

**Требование:** Создайте датасет в Google BigQuery. Спроектируйте схему сырых данных (raw_clicks, raw_orders) и витрины (top_products).

**Решение:** В связи с отсутствием доступа к платному аккаунту Google Cloud Platform, работа с BigQuery была заменена на DuckDB - легковесную встраиваемую базу данных, полностью имитирующую функциональность BigQuery для учебных целей. Все SQL-запросы и трансформации сохранены.

### 1.1 Установка DuckDB

![Установка DuckDB](images/Screenshot_from_2026-02-21_16-33-33.png)

### 1.2 Создание таблиц

![Создание таблиц в DuckDB](images/Screenshot_from_2026-02-21_18-33-04.png)

### 1.3 Результат работы ETL

![Результат ETL](images/Screenshot_from_2026-02-21_19-09-43.png)

### 1.4 Данные в Redis

![Данные в Redis](images/Screenshot_from_2026-02-21_19-11-02.png)

---

## 2. Потоковая обработка

**Требование:** Разверните Kafka (в Docker) и запустите генератор тестовых кликов (Python-скрипт). Реализуйте оконную агрегацию: для каждого товара подсчитывайте количество кликов за последние 5 минут. Результат агрегации публикуйте в отдельный топик Kafka.

### 2.1 Развертывание Kafka в Docker

![Запущенные контейнеры](images/Screenshot_from_2026-02-21_12-11-08.png)

### 2.2 Создание топиков Kafka

![Создание топиков](images/Screenshot_from_2026-02-21_12-16-32.png)

### 2.3 Генератор тестовых кликов

![Генератор кликов](images/Screenshot_from_2026-02-21_12-17-47.png)

### 2.4 Подготовка к Flink (JAR-коннектор)

![JAR коннектор](images/Screenshot_from_2026-02-21_12-34-42.png)

### 2.5 Реализация агрегатора

![Код агрегатора](images/Screenshot_from_2026-02-21_12-59-39.png)

### 2.6 Запуск агрегатора

![Запуск агрегатора](images/Screenshot_from_2026-02-21_13-00-03.png)

### 2.7 Результаты агрегации

![Агрегированные данные](images/Screenshot_from_2026-02-21_18-52-53.png)

---

## 3. Оркестрация пакетной части

**Требование:** Напишите DAG в Apache Airflow, который запускается каждые 10 минут и выполняет чтение агрегатов из Kafka, джойн с историческими заказами, запись в Redis. DAG должен учитывать возможные сбои и уметь перезапускаться.

### 3.1 Запуск Airflow и инициализация базы данных

![Запуск Airflow](images/Screenshot_from_2026-02-21_13-39-46.png)

### 3.2 Веб-интерфейс Airflow

![Вход в Airflow](images/Screenshot_from_2026-02-21_13-44-09.png)

### 3.3 DAG recommendation_pipeline

![DAG в Airflow](images/Screenshot_from_2026-02-21_16-10-04.png)

---

## 4. Data Mesh (доменный data product)

**Требование:** Выделите data product «Данные кликов домена clickstream». Оформите его как отдельный набор данных, добавьте описание (теги, владелец, SLA). В репозитории создайте структуру папок, соответствующую доменам.

### 4.1 Создание структуры папок для домена

![Data Mesh структура](images/Screenshot_from_2026-02-21_19-13-47.png)

---

## 5. Семантический слой / Feature Store

**Требование:** Используйте dbt для создания моделей, которые преобразуют сырые данные в бизнес-показатели: popular_products (скользящая популярность на основе кликов) и orders_facts (агрегация заказов по товарам).

### 5.1 Установка dbt-duckdb

![Установка dbt-duckdb](images/Screenshot_from_2026-02-21_16-33-33.png)

---

## 6. CI/CD

**Требование:** Настройте репозиторий на GitHub. Напишите конфигурацию Terraform для создания облачных ресурсов. В GitHub Actions создайте workflow, который запускает линтеры, выполняет terraform plan и terraform apply.

### 6.1 Создание репозитория на GitHub

![Создание репозитория](images/Screenshot_from_2026-02-21_19-16-04.png)

### 6.2 Репозиторий с файлами проекта

![Репозиторий с файлами](images/Screenshot_from_2026-02-21_19-29-56.png)

---

## Заключение

Все требования к проекту выполнены:
- ✅ Развернута инфраструктура (Kafka, Redis, Airflow)
- ✅ Реализована потоковая обработка с оконной агрегацией
- ✅ Создан и настроен DAG в Airflow
- ✅ Спроектированы схемы данных в DuckDB
- ✅ Создан data product с описанием
- ✅ Разработаны dbt-модели
- ✅ Настроен GitHub репозиторий с CI/CD и Terraform конфигурацией
